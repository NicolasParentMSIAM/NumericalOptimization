{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)  - 1st year</h3></center>\n",
    "<hr>\n",
    "<center><h1>Numerical Optimization</h1></center>\n",
    "<center><h2>Lab on Proximal Algorithms</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "The following script will allow you to import *notebooks* as if you imported *python files* and will have to be executed at each time you launch Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import start\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Composite minimization for machine learning.\n",
    "\n",
    "In this lab, we will investigate optimization algorithms over composite functions composed of a smooth and a non-smooth part using the proximal gradient algorithm over a practical problem of machine learning: binary classification using logistic regression.</br>\n",
    "\n",
    "> Read the file `logistic_regression_student.ipynb` that contains the problem explanation and simulators. \n",
    "\n",
    "> Implement the proximal operation linked to $\\ell_1$ norm in `logistic_regression_student.ipynb`. \n",
    "\n",
    "> Implement the proximal gradient algorithm and test yout algorithm below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing notebook from logistic_regression_student.ipynb\n",
      "------------------------------------\n",
      " Something \n",
      "------------------------------------\n",
      "START    -- stepsize = 0.1\n",
      "0.744499181202\n",
      "0.704048040179\n",
      "0.669118141896\n",
      "0.638917024141\n",
      "0.612730702803\n",
      "0.590428344076\n",
      "0.571501795115\n",
      "0.555782912387\n",
      "0.543256531232\n",
      "0.533086418042\n",
      "0.52392228131\n",
      "0.515733880252\n",
      "0.508500990194\n",
      "0.502303356052\n",
      "0.496929609284\n",
      "0.492259520827\n",
      "0.48796051085\n",
      "0.484078294964\n",
      "0.480724312544\n",
      "0.477640133569\n",
      "0.474770747489\n",
      "0.472162519208\n",
      "0.469827869462\n",
      "0.467651993271\n",
      "0.465684247931\n",
      "0.463851924945\n",
      "0.46217356586\n",
      "0.460648006637\n",
      "0.459218011843\n",
      "0.457876184055\n",
      "0.456615830187\n",
      "0.455544956017\n",
      "0.454561150641\n",
      "0.453638747425\n",
      "0.452773136835\n",
      "0.451960125513\n",
      "0.451195892035\n",
      "0.450476948061\n",
      "0.449800104151\n",
      "0.449162439607\n",
      "0.44856127581\n",
      "0.44802112779\n",
      "0.447513161688\n",
      "0.447034443685\n",
      "0.44658303391\n",
      "0.44615738368\n",
      "0.445758587976\n",
      "0.445382340581\n",
      "0.44502719452\n",
      "0.444691809348\n",
      "0.44437494216\n",
      "0.444075439453\n",
      "0.443792229765\n",
      "0.443524317003\n",
      "0.4432715487\n",
      "0.443035910585\n",
      "0.442813110515\n",
      "0.442602373463\n",
      "0.442402976859\n",
      "0.442214246589\n",
      "0.442035553333\n",
      "0.441866471948\n",
      "0.441706815513\n",
      "0.441555500775\n",
      "0.441412050875\n",
      "0.441276019146\n",
      "0.441151128434\n",
      "0.441033879037\n",
      "0.440922593838\n",
      "0.440816939232\n",
      "0.440716602429\n",
      "0.440621289991\n",
      "0.440530726492\n",
      "0.44044465328\n",
      "0.440362827328\n",
      "0.440285020183\n",
      "0.440211016988\n",
      "0.440140615578\n",
      "0.440073625643\n",
      "0.440009867955\n",
      "0.439949173651\n",
      "0.439891383562\n",
      "0.439836347601\n",
      "0.439783924182\n",
      "0.439733979691\n",
      "0.439686387987\n",
      "0.439641029938\n",
      "0.439597792994\n",
      "0.439556570784\n",
      "0.439517262742\n",
      "0.439479773757\n",
      "0.43944401385\n",
      "0.439409897867\n",
      "0.439377345198\n",
      "0.439346428667\n",
      "0.439317923215\n",
      "0.439290722796\n",
      "0.439264763354\n",
      "0.439239984253\n",
      "0.43921632808\n",
      "0.439193740451\n",
      "0.439172169841\n",
      "0.439151567416\n",
      "0.43913188688\n",
      "0.439113084329\n",
      "0.43909511812\n",
      "0.439077948737\n",
      "0.439061538679\n",
      "0.439046473591\n",
      "0.439032252365\n",
      "0.439018668282\n",
      "0.439005690795\n",
      "0.438993290943\n",
      "0.438981441256\n",
      "0.43897011568\n",
      "0.438959289487\n",
      "0.43894893921\n",
      "0.43893904257\n",
      "0.438929578411\n",
      "0.438920526636\n",
      "0.438911868155\n",
      "0.438903584824\n",
      "0.4388956594\n",
      "0.438888075488\n",
      "0.438880817497\n",
      "0.438873870601\n",
      "0.438867220692\n",
      "0.438860854349\n",
      "0.438854758795\n",
      "0.438848921871\n",
      "0.438843331998\n",
      "0.438837978151\n",
      "0.438832849827\n",
      "0.438827937022\n",
      "0.438823230203\n",
      "0.438818720284\n",
      "0.438814398608\n",
      "0.438810256917\n",
      "0.43880628734\n",
      "0.43880248237\n",
      "0.438798834847\n",
      "0.43879533794\n",
      "0.438791985132\n",
      "0.438788770201\n",
      "0.438785687213\n",
      "0.438782730501\n",
      "0.438779894655\n",
      "0.438777174508\n",
      "0.438774565129\n",
      "0.438772061804\n",
      "0.438769660032\n",
      "0.438767355514\n",
      "0.438765144141\n",
      "0.438763021985\n",
      "0.438760985294\n",
      "0.438759030479\n",
      "0.438757154113\n",
      "0.438755352914\n",
      "0.438753623749\n",
      "0.438751963617\n",
      "0.438750369651\n",
      "0.438748860075\n",
      "0.438747453524\n",
      "0.438746103295\n",
      "0.438744807018\n",
      "0.438743562427\n",
      "0.43874236736\n",
      "0.438741219746\n",
      "0.43874011761\n",
      "0.438739059061\n",
      "0.438738042292\n",
      "0.438737065573\n",
      "0.438736127252\n",
      "0.438735225748\n",
      "0.438734359548\n",
      "0.438733527204\n",
      "0.438732727331\n",
      "0.438731958605\n",
      "0.438731219756\n",
      "0.43873050957\n",
      "0.438729826885\n",
      "0.438729170587\n",
      "0.438728539611\n",
      "0.438727932936\n",
      "0.438727349585\n",
      "0.438726788621\n",
      "0.438726249148\n",
      "0.438725730305\n",
      "0.43872523127\n",
      "0.438724751253\n",
      "0.438724289499\n",
      "0.438723845283\n",
      "0.438723417909\n",
      "0.438723006713\n",
      "0.438722611055\n",
      "0.438722230324\n",
      "0.438721863932\n",
      "0.438721511317\n",
      "0.438721171939\n",
      "0.438720845281\n",
      "0.438720530844\n",
      "0.438720228154\n",
      "0.438719936753\n",
      "0.438719656202\n",
      "0.438719386081\n",
      "0.438719125986\n",
      "0.43871887553\n",
      "0.438718634339\n",
      "0.438718402058\n",
      "0.438718178344\n",
      "0.438717962867\n",
      "0.438717755312\n",
      "0.438717555376\n",
      "0.438717362767\n",
      "0.438717177206\n",
      "0.438716998425\n",
      "0.438716826165\n",
      "0.438716660178\n",
      "0.438716500227\n",
      "0.438716346084\n",
      "0.438716197528\n",
      "0.438716054349\n",
      "0.438715916344\n",
      "0.438715783319\n",
      "0.438715655086\n",
      "0.438715531465\n",
      "0.438715412284\n",
      "0.438715297376\n",
      "0.438715186582\n",
      "0.438715079748\n",
      "0.438714976728\n",
      "0.438714877378\n",
      "0.438714781564\n",
      "0.438714689152\n",
      "0.438714600019\n",
      "0.438714514042\n",
      "0.438714431105\n",
      "0.438714351095\n",
      "0.438714273906\n",
      "0.438714199433\n",
      "0.438714127576\n",
      "0.43871405824\n",
      "0.438713991333\n",
      "0.438713926766\n",
      "0.438713864452\n",
      "0.438713804311\n",
      "0.438713746263\n",
      "0.438713690232\n",
      "0.438713636144\n",
      "0.438713583931\n",
      "0.438713533522\n",
      "0.438713484854\n",
      "0.438713437864\n",
      "0.43871339249\n",
      "0.438713348676\n",
      "0.438713306365\n",
      "0.438713265502\n",
      "0.438713226037\n",
      "0.438713187919\n",
      "0.438713151099\n",
      "0.438713115533\n",
      "0.438713081175\n",
      "0.438713047981\n",
      "0.438713015912\n",
      "0.438712984926\n",
      "0.438712954986\n",
      "0.438712926055\n",
      "0.438712898096\n",
      "0.438712871076\n",
      "0.438712844962\n",
      "0.438712819722\n",
      "0.438712795325\n",
      "0.438712771741\n",
      "0.438712748942\n",
      "0.438712726902\n",
      "0.438712705592\n",
      "0.438712684988\n",
      "0.438712665066\n",
      "0.438712645801\n",
      "0.438712627171\n",
      "0.438712609154\n",
      "0.438712591728\n",
      "0.438712574874\n",
      "0.438712558571\n",
      "0.438712542801\n",
      "0.438712527545\n",
      "0.438712512786\n",
      "0.438712498506\n",
      "0.43871248469\n",
      "0.438712471321\n",
      "0.438712458384\n",
      "0.438712445864\n",
      "0.438712433748\n",
      "0.438712422021\n",
      "0.438712410671\n",
      "0.438712399685\n",
      "0.43871238905\n",
      "0.438712378754\n",
      "0.438712368787\n",
      "0.438712359136\n",
      "0.438712349792\n",
      "0.438712340745\n",
      "0.438712331983\n",
      "0.438712323499\n",
      "0.438712315281\n",
      "0.438712307323\n",
      "0.438712299614\n",
      "0.438712292147\n",
      "0.438712284914\n",
      "0.438712277907\n",
      "0.438712271119\n",
      "0.438712264542\n",
      "0.438712258169\n",
      "0.438712251994\n",
      "0.43871224601\n",
      "0.438712240211\n",
      "0.438712234591\n",
      "0.438712229144\n",
      "0.438712223864\n",
      "0.438712218747\n",
      "0.438712213786\n",
      "0.438712208977\n",
      "0.438712204315\n",
      "0.438712199794\n",
      "0.438712195412\n",
      "0.438712191162\n",
      "0.438712187041\n",
      "0.438712183044\n",
      "0.438712179169\n",
      "0.43871217541\n",
      "0.438712171764\n",
      "0.438712168227\n",
      "0.438712164797\n",
      "0.438712161469\n",
      "0.438712158241\n",
      "0.438712155108\n",
      "0.438712152069\n",
      "0.438712149121\n",
      "0.438712146259\n",
      "0.438712143483\n",
      "0.438712140788\n",
      "0.438712138173\n",
      "0.438712135635\n",
      "0.438712133172\n",
      "0.438712130781\n",
      "0.438712128459\n",
      "0.438712126206\n",
      "0.438712124018\n",
      "0.438712121895\n",
      "0.438712119833\n",
      "0.43871211783\n",
      "0.438712115886\n",
      "0.438712113998\n",
      "0.438712112165\n",
      "0.438712110384\n",
      "0.438712108655\n",
      "0.438712106975\n",
      "0.438712105344\n",
      "0.438712103759\n",
      "0.43871210222\n",
      "0.438712100724\n",
      "0.438712099271\n",
      "0.43871209786\n",
      "0.438712096488\n",
      "0.438712095156\n",
      "0.438712093861\n",
      "0.438712092603\n",
      "0.43871209138\n",
      "0.438712090192\n",
      "0.438712089037\n",
      "0.438712087914\n",
      "0.438712086823\n",
      "0.438712085762\n",
      "0.438712084732\n",
      "0.438712083729\n",
      "0.438712082755\n",
      "0.438712081808\n",
      "0.438712080887\n",
      "0.438712079992\n",
      "0.438712079121\n",
      "0.438712078275\n",
      "0.438712077452\n",
      "0.438712076651\n",
      "0.438712075873\n",
      "0.438712075116\n",
      "0.438712074379\n",
      "0.438712073663\n",
      "0.438712072967\n",
      "0.438712072289\n",
      "0.43871207163\n",
      "0.438712070989\n",
      "0.438712070365\n",
      "0.438712069758\n",
      "0.438712069168\n",
      "0.438712068594\n",
      "0.438712068035\n",
      "0.438712067491\n",
      "0.438712066962\n",
      "0.438712066447\n",
      "0.438712065946\n",
      "0.438712065459\n",
      "0.438712064984\n",
      "0.438712064522\n",
      "0.438712064073\n",
      "0.438712063635\n",
      "0.43871206321\n",
      "0.438712062795\n",
      "FINISHED -- 406 iterations / 2.301772s -- final value: 0.382928 at\n",
      "\n",
      "\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -0.00564259  0.08213131  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -0.05690418  0.75641715  0.82407076\n",
      " -0.13430411]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logistic_regression_student as pb\n",
    "reload(pb)\n",
    "\n",
    "x0 = np.ones(pb.n)*0.1\n",
    "step = 0.1 #gamma\n",
    "PREC = 0.00001\n",
    "ITE_MAX = 500\n",
    "\n",
    "##### proximal gradient algorithm\n",
    "x,x_tab = pb.proximal_gradient_algorithm(pb.F , pb.f_grad , pb.g_prox , x0 , step , PREC, ITE_MAX , True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Investigate the decrease of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the support of the vector $x_k$ (i.e. one point for every non-null coordinate of $x_k$) versus the iterations. \n",
    "\n",
    "> What do you notice? Was it expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Regularization path.\n",
    "\n",
    "\n",
    "We saw above that the algorithm *selected* some coordinates as the other get to zero. Considering our machine learning task (see `logistic_regression_student.ipynb`), this translates into the algorithm selecting a subset of the features that will be used for the prediction step.  \n",
    "\n",
    "> Change the parameter $\\lambda_1$ of the problem (`pb.lam1`) in the code above and investigate how it influences the number of selected features.\n",
    "\n",
    "In order to quantify the influence of this feature selection, let us consider the *regularization path* that is the support of the final points obtained by our minimization method versus the value of $\\lambda_1$.\n",
    "\n",
    "> For $\\lambda_1 = 2^{-12},2^{-11}, .. , 2^{1}$, run the proximal gradient algorithm on the obtained problem and store the support of the final point, the prediction performance on the *training set* (`pb.prediction_train`) and on the *testing set* (`pb.prediction_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import logistic_regression_student as pb\n",
    "reload(pb)\n",
    "\n",
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the *regularization path* and look at the feature signification (file `student.txt` or `logistic_regression_student.ipynb`) to see which are the most important features of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the *training* and *testing* accuracies versus the value of $\\lambda_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
